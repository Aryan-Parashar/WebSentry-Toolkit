import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin

# Function to crawl the website
def spider(url):
    print(f"[*] Crawling {url}")
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            links = soup.find_all("a", href=True)
            forms = soup.find_all("form")
            inputs = soup.find_all("input")
            scripts = soup.find_all("script", src=True)
            images = soup.find_all("img", src=True)
            files = soup.find_all(re.compile(r'(href=".*\.(css|js|png|jpg|jpeg|gif|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|rar)'))

            # Print collected links
            print("[+] Links:")
            print_collected_items(links, url)

            # Print forms and inputs
            print("[+] Forms:")
            print_collected_items(forms)

            print("[+] Inputs:")
            print_collected_items(inputs)

            # Print scripts and images
            print("[+] Scripts:")
            print_collected_items(scripts, url)

            print("[+] Images:")
            print_collected_items(images, url)

            # Print file paths
            print("[+] Files:")
            print_collected_items(files)

        else:
            print(f"Error: {response.status_code}")
    except Exception as e:
        print(f"Error: {e}")

# Function to print collected items
def print_collected_items(items, base_url=None):
    for item in items:
        if base_url:
            print(urljoin(base_url, item["href"]))
        else:
            print(item)

# Main function
def main():
    print("Spider Link Parameter Input")
    print("---")

    target_url = input("Enter your target website URL: ")
    if target_url:
        spider(target_url)
    else:
        print("Please enter a valid URL.")

if __name__ == "__main__":
    main()
